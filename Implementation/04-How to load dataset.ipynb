{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset): #torch.utils.data.Dataset 상속\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73,80,75],\n",
    "                       [93,88,93],\n",
    "                       [89,91,90],\n",
    "                       [96,98,100],\n",
    "                       [73,66,70]]\n",
    "        self.y_data = [[152],[185],[180],[196],[142]]\n",
    "    \n",
    "    def __len__(self): # 이 데이터셋의 총 데이터 수\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self,idx): # 어떤 idx를 받았을 때, 그에 상응하는 입출력 데이터 반환\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 2, # 각 minibatch\n",
    "    shuffle=True, # epoch마다 데이터셋을 섞어서 데이터 학습 순서를 학습하지 못하게 함. 꼭 해줄 것!\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20   Batch 1/3   Loss 22611.06640625\n",
      "Epoch    0/20   Batch 2/3   Loss 12325.982421875\n",
      "Epoch    0/20   Batch 3/3   Loss 18898.681640625\n",
      "Epoch    1/20   Batch 1/3   Loss 10402.7578125\n",
      "Epoch    1/20   Batch 2/3   Loss 15251.6689453125\n",
      "Epoch    1/20   Batch 3/3   Loss 15822.9228515625\n",
      "Epoch    2/20   Batch 1/3   Loss 12254.83984375\n",
      "Epoch    2/20   Batch 2/3   Loss 9800.884765625\n",
      "Epoch    2/20   Batch 3/3   Loss 6584.03759765625\n",
      "Epoch    3/20   Batch 1/3   Loss 8481.8291015625\n",
      "Epoch    3/20   Batch 2/3   Loss 6769.1435546875\n",
      "Epoch    3/20   Batch 3/3   Loss 8478.3984375\n",
      "Epoch    4/20   Batch 1/3   Loss 5654.3056640625\n",
      "Epoch    4/20   Batch 2/3   Loss 7297.38720703125\n",
      "Epoch    4/20   Batch 3/3   Loss 3770.226318359375\n",
      "Epoch    5/20   Batch 1/3   Loss 5744.34375\n",
      "Epoch    5/20   Batch 2/3   Loss 4260.41943359375\n",
      "Epoch    5/20   Batch 3/3   Loss 2952.919677734375\n",
      "Epoch    6/20   Batch 1/3   Loss 3673.0234375\n",
      "Epoch    6/20   Batch 2/3   Loss 3518.524658203125\n",
      "Epoch    6/20   Batch 3/3   Loss 3311.83251953125\n",
      "Epoch    7/20   Batch 1/3   Loss 2079.91552734375\n",
      "Epoch    7/20   Batch 2/3   Loss 3087.7783203125\n",
      "Epoch    7/20   Batch 3/3   Loss 3131.940673828125\n",
      "Epoch    8/20   Batch 1/3   Loss 2484.964599609375\n",
      "Epoch    8/20   Batch 2/3   Loss 2002.354248046875\n",
      "Epoch    8/20   Batch 3/3   Loss 1206.9697265625\n",
      "Epoch    9/20   Batch 1/3   Loss 1633.65966796875\n",
      "Epoch    9/20   Batch 2/3   Loss 1762.68017578125\n",
      "Epoch    9/20   Batch 3/3   Loss 1084.076416015625\n",
      "Epoch   10/20   Batch 1/3   Loss 1250.10986328125\n",
      "Epoch   10/20   Batch 2/3   Loss 1367.38671875\n",
      "Epoch   10/20   Batch 3/3   Loss 856.7880859375\n",
      "Epoch   11/20   Batch 1/3   Loss 902.3599243164062\n",
      "Epoch   11/20   Batch 2/3   Loss 1199.4197998046875\n",
      "Epoch   11/20   Batch 3/3   Loss 514.5488891601562\n",
      "Epoch   12/20   Batch 1/3   Loss 729.01123046875\n",
      "Epoch   12/20   Batch 2/3   Loss 826.34375\n",
      "Epoch   12/20   Batch 3/3   Loss 540.2939453125\n",
      "Epoch   13/20   Batch 1/3   Loss 543.1131591796875\n",
      "Epoch   13/20   Batch 2/3   Loss 545.4644775390625\n",
      "Epoch   13/20   Batch 3/3   Loss 633.3759765625\n",
      "Epoch   14/20   Batch 1/3   Loss 447.14190673828125\n",
      "Epoch   14/20   Batch 2/3   Loss 441.575927734375\n",
      "Epoch   14/20   Batch 3/3   Loss 350.26422119140625\n",
      "Epoch   15/20   Batch 1/3   Loss 371.5816955566406\n",
      "Epoch   15/20   Batch 2/3   Loss 373.158447265625\n",
      "Epoch   15/20   Batch 3/3   Loss 146.5497589111328\n",
      "Epoch   16/20   Batch 1/3   Loss 327.7686462402344\n",
      "Epoch   16/20   Batch 2/3   Loss 256.6414794921875\n",
      "Epoch   16/20   Batch 3/3   Loss 106.68624114990234\n",
      "Epoch   17/20   Batch 1/3   Loss 176.1284942626953\n",
      "Epoch   17/20   Batch 2/3   Loss 232.36717224121094\n",
      "Epoch   17/20   Batch 3/3   Loss 177.64501953125\n",
      "Epoch   18/20   Batch 1/3   Loss 225.265380859375\n",
      "Epoch   18/20   Batch 2/3   Loss 106.28309631347656\n",
      "Epoch   18/20   Batch 3/3   Loss 108.81243896484375\n",
      "Epoch   19/20   Batch 1/3   Loss 73.69341278076172\n",
      "Epoch   19/20   Batch 2/3   Loss 162.34878540039062\n",
      "Epoch   19/20   Batch 3/3   Loss 121.72753143310547\n",
      "Epoch   20/20   Batch 1/3   Loss 108.868408203125\n",
      "Epoch   20/20   Batch 2/3   Loss 90.3531494140625\n",
      "Epoch   20/20   Batch 3/3   Loss 56.42500305175781\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1) #Linear(입력 차원, 출력 차원)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) # 모델이 입력을 받아서 출력함\n",
    "    \n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-6)\n",
    "\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader): # enumerate : https://www.daleseo.com/python-enumerate/\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "        #print(prediction)\n",
    "        \n",
    "        loss = F.mse_loss(prediction, y_train)\n",
    "        #print(\"loss : \", loss)\n",
    "        #print(\"-----------\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(loss)\n",
    "        print(f\"Epoch {epoch:4d}/{nb_epochs}   Batch {batch_idx+1}/{len(dataloader)}   Loss {loss.item()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
